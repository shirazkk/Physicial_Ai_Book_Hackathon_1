"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[7292],{472(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>a,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"intro/code-examples","title":"Code Examples for Physical AI","description":"This page provides practical code examples that demonstrate core Physical AI concepts using Python and PyBullet. These examples illustrate the principles discussed in the introduction and provide a foundation for more advanced implementations.","source":"@site/docs/intro/code-examples.md","sourceDirName":"intro","slug":"/intro/code-examples","permalink":"/Physicial_Ai_Book_Hackathon_1/docs/intro/code-examples","draft":false,"unlisted":false,"editUrl":"https://github.com/shirazkk/Physicial_Ai_Book_Hackathon_1/edit/main/my-website/docs/intro/code-examples.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"sidebar_position":8,"title":"Code Examples for Physical AI"},"sidebar":"tutorialSidebar","previous":{"title":"Skills and Mindset for Physical AI","permalink":"/Physicial_Ai_Book_Hackathon_1/docs/intro/skills-mindset"},"next":{"title":"Cross-References and Links","permalink":"/Physicial_Ai_Book_Hackathon_1/docs/intro/cross-references"}}');var s=t(4848),i=t(8453);const a={sidebar_position:8,title:"Code Examples for Physical AI"},r="Code Examples for Physical AI",l={},d=[{value:"Basic Physical AI Concepts in Code",id:"basic-physical-ai-concepts-in-code",level:2},{value:"1. Robot State Representation",id:"1-robot-state-representation",level:3},{value:"2. Sensor Data Processing",id:"2-sensor-data-processing",level:3},{value:"3. Control Systems for Physical AI",id:"3-control-systems-for-physical-ai",level:3},{value:"4. Simulation-First Approach with PyBullet",id:"4-simulation-first-approach-with-pybullet",level:3},{value:"5. Embodied Intelligence in Practice",id:"5-embodied-intelligence-in-practice",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Next Steps",id:"next-steps",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"code-examples-for-physical-ai",children:"Code Examples for Physical AI"})}),"\n",(0,s.jsx)(n.p,{children:"This page provides practical code examples that demonstrate core Physical AI concepts using Python and PyBullet. These examples illustrate the principles discussed in the introduction and provide a foundation for more advanced implementations."}),"\n",(0,s.jsx)(n.h2,{id:"basic-physical-ai-concepts-in-code",children:"Basic Physical AI Concepts in Code"}),"\n",(0,s.jsx)(n.h3,{id:"1-robot-state-representation",children:"1. Robot State Representation"}),"\n",(0,s.jsx)(n.p,{children:"In Physical AI, robots are represented by their state in the physical world:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import numpy as np\n\nclass RobotState:\n    def __init__(self, position, orientation, joint_angles, velocities):\n        """\n        Represents the state of a robot in the physical world\n        """\n        self.position = np.array(position)  # 3D position [x, y, z]\n        self.orientation = np.array(orientation)  # 4D quaternion [w, x, y, z]\n        self.joint_angles = np.array(joint_angles)  # Joint positions\n        self.velocities = np.array(velocities)  # Joint velocities\n\n    def update_position(self, new_position):\n        """Update the robot\'s position in the environment"""\n        self.position = np.array(new_position)\n\n    def get_configuration_space(self):\n        """Return the robot\'s configuration in joint space"""\n        return self.joint_angles\n\n    def get_task_space(self):\n        """Return the robot\'s position in Cartesian space"""\n        return self.position\n'})}),"\n",(0,s.jsx)(n.h3,{id:"2-sensor-data-processing",children:"2. Sensor Data Processing"}),"\n",(0,s.jsx)(n.p,{children:"Physical AI systems must process sensor data to understand their environment:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import numpy as np\n\nclass SensorProcessor:\n    def __init__(self):\n        self.sensor_noise = 0.01  # Standard deviation of sensor noise\n\n    def process_lidar_data(self, raw_data):\n        """\n        Process raw LIDAR data to detect obstacles\n        """\n        # Apply noise model to simulate real-world sensor noise\n        noisy_data = raw_data + np.random.normal(0, self.sensor_noise, raw_data.shape)\n\n        # Filter out distances beyond maximum range\n        valid_distances = noisy_data[noisy_data < 10.0]  # 10m max range\n\n        # Identify potential obstacles\n        obstacles = []\n        for i, distance in enumerate(valid_distances):\n            if distance < 0.5:  # Obstacle within 50cm\n                obstacles.append({\n                    \'angle\': i * (2 * np.pi / len(raw_data)),\n                    \'distance\': distance\n                })\n\n        return obstacles\n\n    def process_camera_data(self, image):\n        """\n        Process camera image to detect objects\n        """\n        # Simple edge detection (in practice, you\'d use more sophisticated methods)\n        edges = self.detect_edges(image)\n\n        # Identify objects based on edge patterns\n        objects = self.identify_objects_from_edges(edges)\n\n        return objects\n\n    def detect_edges(self, image):\n        """Simple edge detection"""\n        # In practice, you\'d use more sophisticated techniques like Canny edge detection\n        return np.gradient(image)\n\n    def identify_objects_from_edges(self, edges):\n        """Identify objects based on edge patterns"""\n        # Simplified object identification\n        objects = []\n        # In practice, you\'d use computer vision techniques like contour detection\n        return objects\n'})}),"\n",(0,s.jsx)(n.h3,{id:"3-control-systems-for-physical-ai",children:"3. Control Systems for Physical AI"}),"\n",(0,s.jsx)(n.p,{children:"Implementing control systems that operate in real-time with physical constraints:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport time\n\nclass PIDController:\n    def __init__(self, kp, ki, kd, dt=0.01):\n        """\n        PID controller for robot control\n        kp: Proportional gain\n        ki: Integral gain\n        kd: Derivative gain\n        dt: Time step\n        """\n        self.kp = kp\n        self.ki = ki\n        self.kd = kd\n        self.dt = dt\n\n        self.previous_error = 0\n        self.integral = 0\n\n    def compute(self, target, current):\n        """\n        Compute control output based on target and current values\n        """\n        error = target - current\n\n        # Proportional term\n        p_term = self.kp * error\n\n        # Integral term\n        self.integral += error * self.dt\n        i_term = self.ki * self.integral\n\n        # Derivative term\n        derivative = (error - self.previous_error) / self.dt\n        d_term = self.kd * derivative\n\n        # Store error for next iteration\n        self.previous_error = error\n\n        # Return control output\n        return p_term + i_term + d_term\n\nclass RobotController:\n    def __init__(self):\n        # Initialize PID controllers for different joints\n        self.joint_controllers = {\n            \'joint_1\': PIDController(kp=2.0, ki=0.1, kd=0.01),\n            \'joint_2\': PIDController(kp=2.0, ki=0.1, kd=0.01),\n            \'joint_3\': PIDController(kp=2.0, ki=0.1, kd=0.01)\n        }\n\n    def control_step(self, target_positions, current_positions):\n        """\n        Compute control commands for all joints\n        """\n        control_outputs = {}\n\n        for joint_name, target_pos in target_positions.items():\n            current_pos = current_positions.get(joint_name, 0)\n            controller = self.joint_controllers[joint_name]\n\n            control_output = controller.compute(target_pos, current_pos)\n            control_outputs[joint_name] = control_output\n\n        return control_outputs\n'})}),"\n",(0,s.jsx)(n.h3,{id:"4-simulation-first-approach-with-pybullet",children:"4. Simulation-First Approach with PyBullet"}),"\n",(0,s.jsx)(n.p,{children:"Implementing the simulation-first methodology:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import pybullet as p\nimport pybullet_data\nimport numpy as np\n\nclass SimulationEnvironment:\n    def __init__(self, use_gui=True):\n        """\n        Initialize PyBullet simulation environment\n        """\n        if use_gui:\n            self.physics_client = p.connect(p.GUI)\n        else:\n            self.physics_client = p.connect(p.DIRECT)\n\n        # Set gravity\n        p.setGravity(0, 0, -9.81)\n\n        # Load plane\n        p.setAdditionalSearchPath(pybullet_data.getDataPath())\n        self.plane_id = p.loadURDF("plane.urdf")\n\n        # Robot ID placeholder\n        self.robot_id = None\n\n    def load_robot(self, urdf_path, start_position=[0, 0, 1]):\n        """\n        Load robot into the simulation\n        """\n        self.robot_id = p.loadURDF(urdf_path, start_position)\n        return self.robot_id\n\n    def get_robot_state(self):\n        """\n        Get current state of the robot\n        """\n        if self.robot_id is None:\n            return None\n\n        # Get base position and orientation\n        pos, orn = p.getBasePositionAndOrientation(self.robot_id)\n\n        # Get joint states\n        joint_states = []\n        for i in range(p.getNumJoints(self.robot_id)):\n            joint_info = p.getJointState(self.robot_id, i)\n            joint_states.append({\n                \'position\': joint_info[0],\n                \'velocity\': joint_info[1],\n                \'force\': joint_info[3]\n            })\n\n        return {\n            \'position\': pos,\n            \'orientation\': orn,\n            \'joint_states\': joint_states\n        }\n\n    def apply_control_commands(self, joint_commands):\n        """\n        Apply control commands to robot joints\n        """\n        for joint_idx, command in enumerate(joint_commands):\n            p.setJointMotorControl2(\n                bodyUniqueId=self.robot_id,\n                jointIndex=joint_idx,\n                controlMode=p.POSITION_CONTROL,\n                targetPosition=command\n            )\n\n    def step_simulation(self):\n        """\n        Step the simulation forward\n        """\n        p.stepSimulation()\n\n    def disconnect(self):\n        """\n        Disconnect from physics server\n        """\n        p.disconnect(self.physics_client)\n\n# Example usage of simulation environment\ndef example_simulation_usage():\n    """\n    Example of how to use the simulation environment\n    """\n    # Create simulation environment\n    sim_env = SimulationEnvironment(use_gui=True)\n\n    # Load a simple robot (using KUKA iiwa as example)\n    robot_id = sim_env.load_robot("kuka_iiwa/model.urdf")\n\n    # Run simulation for a while\n    for i in range(1000):  # 1000 steps\n        # Get current robot state\n        state = sim_env.get_robot_state()\n\n        # Simple control: move to a target position\n        target_positions = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n        sim_env.apply_control_commands(target_positions)\n\n        # Step simulation\n        sim_env.step_simulation()\n\n        # Small delay to visualize\n        if i % 100 == 0:  # Print every 100 steps\n            print(f"Step {i}: Robot position = {state[\'position\'] if state else \'N/A\'}")\n\n    # Clean up\n    sim_env.disconnect()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"5-embodied-intelligence-in-practice",children:"5. Embodied Intelligence in Practice"}),"\n",(0,s.jsx)(n.p,{children:"Implementing concepts of embodied intelligence:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import numpy as np\n\nclass EmbodiedAgent:\n    def __init__(self, body_properties, environment_properties):\n        """\n        An agent that embodies intelligence through its interaction with the environment\n        """\n        self.body = body_properties  # Properties of the agent\'s body\n        self.environment = environment_properties  # Properties of the environment\n        self.state = np.zeros(10)  # Internal state representation\n        self.sensors = []\n        self.actuators = []\n\n    def perceive(self, environment_state):\n        """\n        Agent perceives the environment through its sensors\n        """\n        # In embodied cognition, perception is active and guided by motor intentions\n        sensor_data = self.process_environment_state(environment_state)\n\n        # Update internal state based on perception\n        self.update_internal_state(sensor_data)\n\n        return sensor_data\n\n    def act(self, sensor_data):\n        """\n        Agent acts on the environment through its actuators\n        """\n        # Decision making based on sensor data and internal state\n        motor_commands = self.decision_process(sensor_data)\n\n        # Execute actions through actuators\n        self.execute_motor_commands(motor_commands)\n\n        return motor_commands\n\n    def process_environment_state(self, env_state):\n        """\n        Process environmental information through the agent\'s body\n        """\n        # The agent\'s body affects what can be perceived\n        # For example, sensor positions determine what can be sensed\n        processed_data = {}\n\n        for sensor in self.sensors:\n            # Each sensor provides data based on its position and the environment\n            sensor_data = sensor.sense(env_state, self.body.position)\n            processed_data[sensor.name] = sensor_data\n\n        return processed_data\n\n    def decision_process(self, sensor_data):\n        """\n        Decision process that considers the agent\'s embodiment\n        """\n        # In embodied cognition, decision making is influenced by the body\n        # The agent\'s physical constraints affect what actions are possible\n\n        # Simple example: if obstacle detected in front, move around it\n        if \'front_sensor\' in sensor_data and sensor_data[\'front_sensor\'] < 0.5:\n            # Body configuration affects how to move around obstacle\n            if self.body.wheel_base > 0.3:  # Wheeled robot\n                return {\'left_motor\': -0.5, \'right_motor\': 0.5}  # Turn right\n            else:  # Legged robot\n                return {\'left_leg\': 0.1, \'right_leg\': -0.1}  # Step aside\n\n        return {}  # No action needed\n\n    def update_internal_state(self, sensor_data):\n        """\n        Update internal state based on sensory input\n        """\n        # In embodied cognition, internal state is shaped by interaction\n        # with the environment through the body\n        pass\n\n    def execute_motor_commands(self, commands):\n        """\n        Execute motor commands through actuators\n        """\n        for actuator_name, command in commands.items():\n            actuator = next((a for a in self.actuators if a.name == actuator_name), None)\n            if actuator:\n                actuator.execute(command)\n\n# Example: Creating an embodied agent\ndef create_simple_embodied_agent():\n    """\n    Create a simple embodied agent to demonstrate the concept\n    """\n    body_props = {\n        \'position\': [0, 0, 0],\n        \'wheel_base\': 0.3,  # Distance between wheels\n        \'mass\': 1.0,\n        \'size\': [0.2, 0.2, 0.1]  # width, length, height\n    }\n\n    env_props = {\n        \'size\': [10, 10],  # 10x10 environment\n        \'obstacles\': [{\'position\': [2, 2], \'size\': [0.5, 0.5]}]\n    }\n\n    agent = EmbodiedAgent(body_props, env_props)\n\n    # Add simple sensors and actuators\n    class SimpleSensor:\n        def __init__(self, name, position):\n            self.name = name\n            self.position = position\n\n        def sense(self, env_state, agent_pos):\n            # Simple distance measurement\n            obstacle_pos = env_props[\'obstacles\'][0][\'position\']\n            distance = np.linalg.norm(np.array(obstacle_pos) - np.array(agent_pos))\n            return distance\n\n    class SimpleActuator:\n        def __init__(self, name):\n            self.name = name\n\n        def execute(self, command):\n            # Execute the command (in simulation, this would affect the physics)\n            pass\n\n    agent.sensors = [SimpleSensor(\'front_sensor\', [0.1, 0, 0])]\n    agent.actuators = [SimpleActuator(\'left_motor\'), SimpleActuator(\'right_motor\')]\n\n    return agent\n'})}),"\n",(0,s.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Embodiment Matters"}),": In Physical AI, the body is not just a vessel but an integral part of the cognitive system"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time Processing"}),": Physical systems must operate within real-time constraints"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Uncertainty Management"}),": Physical systems must handle sensor noise and environmental uncertainty"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simulation-First"}),": Develop and test in simulation before real-world deployment"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety-Critical"}),": Physical AI systems must prioritize safety in all operations"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"These code examples provide a foundation for understanding how Physical AI concepts translate into practical implementations. Each example demonstrates a key principle while maintaining the simulation-first approach that is central to safe and effective Physical AI development."}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Experiment with the provided PyBullet examples in your own environment"}),"\n",(0,s.jsx)(n.li,{children:"Modify the code examples to explore different scenarios"}),"\n",(0,s.jsx)(n.li,{children:"Apply these concepts to the four-module structure outlined in the book"}),"\n",(0,s.jsx)(n.li,{children:"Consider how these implementations would need to be adapted for real-world deployment"}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453(e,n,t){t.d(n,{R:()=>a,x:()=>r});var o=t(6540);const s={},i=o.createContext(s);function a(e){const n=o.useContext(i);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);