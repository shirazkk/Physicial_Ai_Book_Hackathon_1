"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[4493],{825(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>r,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"module-1-foundation/chapter3-sensing-perception","title":"Chapter 3: Sensing and Perception for Physical AI & Robotics","description":"Learning Objectives","source":"@site/docs/module-1-foundation/chapter3-sensing-perception.md","sourceDirName":"module-1-foundation","slug":"/module-1-foundation/chapter3-sensing-perception","permalink":"/Physicial_Ai_Book_Hackathon_1/docs/module-1-foundation/chapter3-sensing-perception","draft":false,"unlisted":false,"editUrl":"https://github.com/shirazkk/Physicial_Ai_Book_Hackathon_1/edit/main/my-website/docs/module-1-foundation/chapter3-sensing-perception.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: Kinematics and Dynamics for Physical AI & Robotics","permalink":"/Physicial_Ai_Book_Hackathon_1/docs/module-1-foundation/chapter2-kinematics-dynamics"},"next":{"title":"Chapter 4: Embodied Intelligence for Physical AI & Robotics","permalink":"/Physicial_Ai_Book_Hackathon_1/docs/module-1-foundation/chapter4-embodied-intelligence"}}');var s=i(4848),a=i(8453);const r={},o="Chapter 3: Sensing and Perception for Physical AI & Robotics",l={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Introduction",id:"introduction",level:2},{value:"1. Sensor Types and Characteristics",id:"1-sensor-types-and-characteristics",level:2},{value:"1.1 Proprioceptive Sensors",id:"11-proprioceptive-sensors",level:3},{value:"Encoders",id:"encoders",level:4},{value:"Inertial Measurement Units (IMUs)",id:"inertial-measurement-units-imus",level:4},{value:"1.2 Exteroceptive Sensors",id:"12-exteroceptive-sensors",level:3},{value:"Range Sensors",id:"range-sensors",level:4},{value:"Cameras",id:"cameras",level:4},{value:"2. Sensor Fusion Techniques",id:"2-sensor-fusion-techniques",level:2},{value:"2.1 Weighted Average Fusion",id:"21-weighted-average-fusion",level:3},{value:"2.2 Covariance Intersection",id:"22-covariance-intersection",level:3},{value:"3. State Estimation and Filtering Methods",id:"3-state-estimation-and-filtering-methods",level:2},{value:"3.1 Kalman Filter",id:"31-kalman-filter",level:3},{value:"3.2 Extended Kalman Filter (EKF)",id:"32-extended-kalman-filter-ekf",level:3},{value:"3.3 Particle Filter",id:"33-particle-filter",level:3},{value:"4. PyBullet Examples for Perception",id:"4-pybullet-examples-for-perception",level:2},{value:"5. Sensor Fusion Exercise",id:"5-sensor-fusion-exercise",level:2},{value:"Exercise 1: Multi-Sensor Position Estimation",id:"exercise-1-multi-sensor-position-estimation",level:3},{value:"6. Summary",id:"6-summary",level:2},{value:"7. Implementation Guide",id:"7-implementation-guide",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"chapter-3-sensing-and-perception-for-physical-ai--robotics",children:"Chapter 3: Sensing and Perception for Physical AI & Robotics"})}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this chapter, readers will be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Identify different types of sensors used in robotics and their characteristics"}),"\n",(0,s.jsx)(n.li,{children:"Apply sensor fusion techniques to combine information from multiple sensors"}),"\n",(0,s.jsx)(n.li,{children:"Implement state estimation and filtering methods for robotic perception"}),"\n",(0,s.jsx)(n.li,{children:"Design effective perception systems for robotic applications"}),"\n",(0,s.jsx)(n.li,{children:"Use probabilistic methods for handling uncertainty in sensor data"}),"\n",(0,s.jsx)(n.li,{children:"Implement perception examples using PyBullet simulation"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understanding of probability and statistics (covered in Chapter 1)"}),"\n",(0,s.jsx)(n.li,{children:"Basic knowledge of linear algebra (covered in Chapter 1)"}),"\n",(0,s.jsx)(n.li,{children:"Fundamental concepts of robotics and coordinate systems"}),"\n",(0,s.jsx)(n.li,{children:"Basic Python programming skills"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(n.p,{children:"Sensing and perception are critical components of robotic systems, enabling robots to understand and interact with their environment. Robots rely on various sensors to gather information about their state and surroundings, then process this information to make informed decisions. This chapter covers sensor types, characteristics, fusion techniques, and state estimation methods essential for building effective robotic perception systems."}),"\n",(0,s.jsx)(n.h2,{id:"1-sensor-types-and-characteristics",children:"1. Sensor Types and Characteristics"}),"\n",(0,s.jsx)(n.p,{children:"Robots use various sensors to perceive their environment and internal state. Each sensor type has specific characteristics that make it suitable for particular applications."}),"\n",(0,s.jsx)(n.h3,{id:"11-proprioceptive-sensors",children:"1.1 Proprioceptive Sensors"}),"\n",(0,s.jsx)(n.p,{children:"Proprioceptive sensors measure the internal state of the robot."}),"\n",(0,s.jsx)(n.h4,{id:"encoders",children:"Encoders"}),"\n",(0,s.jsx)(n.p,{children:"Encoders measure joint positions and velocities."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport matplotlib.pyplot as plt\n\nclass Encoder:\n    def __init__(self, resolution=1000, noise_level=0.01):\n        """\n        Encoder simulator\n        resolution: counts per revolution\n        noise_level: standard deviation of noise as fraction of signal\n        """\n        self.resolution = resolution\n        self.noise_level = noise_level\n        self.counts_per_rev = resolution\n        self.position = 0\n        self.velocity = 0\n\n    def read_position(self, true_position, add_noise=True):\n        """Read encoder position with optional noise"""\n        # Convert position to encoder counts\n        counts = int(true_position * self.counts_per_rev / (2 * np.pi))\n\n        # Add noise if requested\n        if add_noise:\n            noise = np.random.normal(0, self.noise_level * self.counts_per_rev)\n            counts += noise\n\n        # Convert back to position\n        measured_position = counts * 2 * np.pi / self.counts_per_rev\n        return measured_position\n\n    def read_velocity(self, true_velocity, add_noise=True):\n        """Read encoder velocity with optional noise"""\n        if add_noise:\n            noise = np.random.normal(0, self.noise_level * abs(true_velocity))\n            return true_velocity + noise\n        return true_velocity\n\n# Example: Simulate encoder readings\nencoder = Encoder(resolution=4096, noise_level=0.005)\n\n# Simulate a rotating joint\ntime = np.linspace(0, 2, 100)\ntrue_positions = np.sin(time * np.pi)  # Sinusoidal motion\ntrue_velocities = np.pi * np.cos(time * np.pi)  # Derivative\n\nmeasured_positions = [encoder.read_position(pos) for pos in true_positions]\nmeasured_velocities = [encoder.read_velocity(vel) for vel in true_velocities]\n\nprint(f"Encoder resolution: {encoder.resolution} counts/rev")\nprint(f"Sample measurements - True pos: {true_positions[10]:.3f}, Measured: {measured_positions[10]:.3f}")\n'})}),"\n",(0,s.jsx)(n.h4,{id:"inertial-measurement-units-imus",children:"Inertial Measurement Units (IMUs)"}),"\n",(0,s.jsx)(n.p,{children:"IMUs measure acceleration, angular velocity, and sometimes magnetic field."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class IMU:\n    def __init__(self, accel_noise=0.01, gyro_noise=0.001, mag_noise=0.01):\n        """\n        IMU simulator\n        """\n        self.accel_noise = accel_noise  # m/s\xb2\n        self.gyro_noise = gyro_noise    # rad/s\n        self.mag_noise = mag_noise      # arbitrary units\n        self.gravity = 9.81  # m/s\xb2\n\n    def read_accelerometer(self, true_acceleration, add_noise=True):\n        """Read accelerometer data with noise"""\n        if add_noise:\n            noise = np.random.normal(0, self.accel_noise, size=true_acceleration.shape)\n            return true_acceleration + noise\n        return true_acceleration\n\n    def read_gyroscope(self, true_angular_velocity, add_noise=True):\n        """Read gyroscope data with noise"""\n        if add_noise:\n            noise = np.random.normal(0, self.gyro_noise, size=true_angular_velocity.shape)\n            return true_angular_velocity + noise\n        return true_angular_velocity\n\n    def read_magnetometer(self, true_magnetic_field, add_noise=True):\n        """Read magnetometer data with noise"""\n        if add_noise:\n            noise = np.random.normal(0, self.mag_noise, size=true_magnetic_field.shape)\n            return true_magnetic_field + noise\n        return true_magnetic_field\n\n# Example: Simulate IMU readings\nimu = IMU(accel_noise=0.02, gyro_noise=0.002, mag_noise=0.02)\n\n# Simulate robot movement\ntrue_accel = np.array([0.5, 0.2, 9.81])  # x, y, z acceleration (including gravity)\ntrue_ang_vel = np.array([0.1, -0.05, 0.02])  # Angular velocities\ntrue_mag_field = np.array([0.2, 0.1, 0.5])  # Magnetic field components\n\nmeasured_accel = imu.read_accelerometer(true_accel)\nmeasured_gyro = imu.read_gyroscope(true_ang_vel)\nmeasured_mag = imu.read_magnetometer(true_mag_field)\n\nprint(f"Accelerometer - True: {true_accel}, Measured: {measured_accel}")\nprint(f"Gyroscope - True: {true_ang_vel}, Measured: {measured_gyro}")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"12-exteroceptive-sensors",children:"1.2 Exteroceptive Sensors"}),"\n",(0,s.jsx)(n.p,{children:"Exteroceptive sensors measure properties of the external environment."}),"\n",(0,s.jsx)(n.h4,{id:"range-sensors",children:"Range Sensors"}),"\n",(0,s.jsx)(n.p,{children:"Range sensors measure distances to objects in the environment."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class RangeSensor:\n    def __init__(self, max_range=10.0, min_range=0.1, accuracy=0.01, fov=30):\n        """\n        Range sensor simulator (e.g., ultrasonic, IR, LiDAR)\n        """\n        self.max_range = max_range\n        self.min_range = min_range\n        self.accuracy = accuracy  # measurement accuracy\n        self.fov = fov  # field of view in degrees\n\n    def measure_distance(self, true_distance, add_noise=True):\n        """Measure distance with sensor limitations and noise"""\n        # Check if within range\n        if true_distance > self.max_range:\n            return float(\'inf\')  # Out of range\n        elif true_distance < self.min_range:\n            return self.min_range  # Too close\n\n        if add_noise:\n            noise = np.random.normal(0, self.accuracy)\n            measured = true_distance + noise\n            # Ensure within bounds\n            measured = max(self.min_range, min(self.max_range, measured))\n            return measured\n        return true_distance\n\n    def detect_object(self, true_distance, threshold=None):\n        """Detect if an object is within range"""\n        if threshold is None:\n            threshold = self.max_range\n        measured_dist = self.measure_distance(true_distance)\n        return measured_dist < threshold and measured_dist != float(\'inf\')\n\n# Example: Simulate range sensor readings\nrange_sensor = RangeSensor(max_range=5.0, min_range=0.05, accuracy=0.02)\n\n# Simulate measurements at different distances\ndistances = [0.5, 1.0, 2.0, 4.0, 6.0, 0.02]\nfor dist in distances:\n    measured = range_sensor.measure_distance(dist)\n    detected = range_sensor.detect_object(dist)\n    print(f"True: {dist:.2f}m -> Measured: {measured:.2f}m, Detected: {detected}")\n'})}),"\n",(0,s.jsx)(n.h4,{id:"cameras",children:"Cameras"}),"\n",(0,s.jsx)(n.p,{children:"Cameras provide rich visual information about the environment."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class Camera:\n    def __init__(self, width=640, height=480, fov_h=60, fov_v=45, focal_length=500):\n        """\n        Camera simulator\n        """\n        self.width = width\n        self.height = height\n        self.fov_h = fov_h  # horizontal field of view in degrees\n        self.fov_v = fov_v  # vertical field of view in degrees\n        self.focal_length = focal_length\n        self.cx = width / 2  # principal point x\n        self.cy = height / 2  # principal point y\n\n    def world_to_pixel(self, world_point, camera_pose):\n        """\n        Convert 3D world point to 2D pixel coordinates\n        world_point: [x, y, z] in world coordinates\n        camera_pose: [x, y, z, roll, pitch, yaw] pose of camera\n        """\n        # This is a simplified version - in practice, this involves\n        # transforming to camera frame and applying projection\n        x_w, y_w, z_w = world_point\n\n        # For simplicity, assume camera is at origin looking along -z axis\n        # Transform point to camera frame (simplified)\n        x_cam = x_w - camera_pose[0]\n        y_cam = y_w - camera_pose[1]\n        z_cam = z_w - camera_pose[2]\n\n        # Project to image plane\n        if z_cam > 0:  # Point in front of camera\n            x_pix = self.focal_length * x_cam / z_cam + self.cx\n            y_pix = self.focal_length * y_cam / z_cam + self.cy\n            return [x_pix, y_pix]\n        else:\n            return None  # Point behind camera\n\n    def get_depth_at_pixel(self, pixel_coords, depth_map):\n        """\n        Get depth value at specific pixel coordinates\n        """\n        x, y = int(pixel_coords[0]), int(pixel_coords[1])\n        if 0 <= x < self.width and 0 <= y < self.height:\n            return depth_map[y, x]\n        return None\n\n# Example: Simulate camera projection\ncamera = Camera(width=320, height=240, fov_h=60, fov_v=45)\n\n# Simulate a point in the world\nworld_point = [1.0, 0.5, 2.0]  # x, y, z in meters\ncamera_pose = [0, 0, 0, 0, 0, 0]  # x, y, z, roll, pitch, yaw\n\npixel_coords = camera.world_to_pixel(world_point, camera_pose)\nif pixel_coords:\n    print(f"World point {world_point} -> Pixel coordinates {pixel_coords}")\nelse:\n    print(f"World point {world_point} not visible to camera")\n'})}),"\n",(0,s.jsx)(n.h2,{id:"2-sensor-fusion-techniques",children:"2. Sensor Fusion Techniques"}),"\n",(0,s.jsx)(n.p,{children:"Sensor fusion combines information from multiple sensors to improve perception accuracy and robustness."}),"\n",(0,s.jsx)(n.h3,{id:"21-weighted-average-fusion",children:"2.1 Weighted Average Fusion"}),"\n",(0,s.jsx)(n.p,{children:"Simple fusion technique for combining redundant sensor measurements."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def weighted_average_fusion(measurements, uncertainties):\n    """\n    Fuse multiple sensor measurements using weighted average\n    measurements: list of measured values\n    uncertainties: list of uncertainty values (standard deviations)\n    """\n    # Calculate weights (inverse of variance)\n    weights = [1.0 / (unc**2) for unc in uncertainties]\n\n    # Calculate weighted sum\n    weighted_sum = sum(m * w for m, w in zip(measurements, weights))\n    total_weight = sum(weights)\n\n    # Calculate fused estimate\n    fused_estimate = weighted_sum / total_weight\n\n    # Calculate fused uncertainty\n    fused_uncertainty = np.sqrt(1.0 / total_weight)\n\n    return fused_estimate, fused_uncertainty\n\n# Example: Fuse measurements from two sensors\nsensor1_measurement = 10.2\nsensor1_uncertainty = 0.5\n\nsensor2_measurement = 9.8\nsensor2_uncertainty = 0.8\n\nmeasurements = [sensor1_measurement, sensor2_measurement]\nuncertainties = [sensor1_uncertainty, sensor2_uncertainty]\n\nfused_result, fused_unc = weighted_average_fusion(measurements, uncertainties)\n\nprint(f"Sensor 1: {sensor1_measurement} \xb1 {sensor1_uncertainty}")\nprint(f"Sensor 2: {sensor2_measurement} \xb1 {sensor2_uncertainty}")\nprint(f"Fused result: {fused_result:.3f} \xb1 {fused_unc:.3f}")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"22-covariance-intersection",children:"2.2 Covariance Intersection"}),"\n",(0,s.jsx)(n.p,{children:"Method for fusing estimates when cross-correlations are unknown."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def covariance_intersection(mean1, cov1, mean2, cov2):\n    """\n    Fuse two estimates using covariance intersection\n    Handles unknown correlations between estimates\n    """\n    # For scalar case (simple version)\n    if np.isscalar(cov1) and np.isscalar(cov2):\n        # Calculate weights\n        w1 = cov2 / (cov1 + cov2)\n        w2 = cov1 / (cov1 + cov2)\n\n        # Fused estimate\n        fused_mean = w1 * mean1 + w2 * mean2\n\n        # Fused covariance\n        fused_cov = 1.0 / (1.0/cov1 + 1.0/cov2)\n\n        return fused_mean, fused_cov\n\n    # For vector/matrix case (simplified implementation)\n    # In practice, this would involve matrix inversions\n    else:\n        # Calculate weights based on determinant of covariances\n        det1_inv = 1.0 / np.linalg.det(cov1) if np.isscalar(np.linalg.det(cov1)) else 1.0\n        det2_inv = 1.0 / np.linalg.det(cov2) if np.isscalar(np.linalg.det(cov2)) else 1.0\n\n        omega = det1_inv / (det1_inv + det2_inv)\n\n        # Fused estimate\n        fused_mean = omega * mean1 + (1 - omega) * mean2\n\n        # Fused covariance\n        fused_cov = np.linalg.inv(omega * np.linalg.inv(cov1) + (1 - omega) * np.linalg.inv(cov2))\n\n        return fused_mean, fused_cov\n\n# Example: Covariance intersection for scalar values\nmean1, cov1 = 10.2, 0.25  # Variance = 0.25, Std dev = 0.5\nmean2, cov2 = 9.8, 0.64   # Variance = 0.64, Std dev = 0.8\n\nfused_mean_ci, fused_cov_ci = covariance_intersection(mean1, cov1, mean2, cov2)\n\nprint(f"Covariance Intersection:")\nprint(f"Estimate 1: {mean1} \xb1 {np.sqrt(cov1):.3f}")\nprint(f"Estimate 2: {mean2} \xb1 {np.sqrt(cov2):.3f}")\nprint(f"Fused estimate: {fused_mean_ci:.3f} \xb1 {np.sqrt(fused_cov_ci):.3f}")\n'})}),"\n",(0,s.jsx)(n.h2,{id:"3-state-estimation-and-filtering-methods",children:"3. State Estimation and Filtering Methods"}),"\n",(0,s.jsx)(n.p,{children:"State estimation is crucial for maintaining accurate knowledge of robot state despite noisy sensor data."}),"\n",(0,s.jsx)(n.h3,{id:"31-kalman-filter",children:"3.1 Kalman Filter"}),"\n",(0,s.jsx)(n.p,{children:"The Kalman filter is an optimal estimator for linear systems with Gaussian noise."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class KalmanFilter:\n    def __init__(self, dim_x, dim_z, dim_u=0):\n        """\n        Kalman Filter implementation\n        dim_x: dimension of state vector\n        dim_z: dimension of measurement vector\n        dim_u: dimension of control input vector\n        """\n        self.dim_x = dim_x\n        self.dim_z = dim_z\n        self.dim_u = dim_u\n\n        # State vector: [position, velocity]\n        self.x = np.zeros((dim_x, 1))\n\n        # State covariance matrix\n        self.P = np.eye(dim_x) * 1000\n\n        # Process noise covariance\n        self.Q = np.eye(dim_x)\n\n        # Measurement noise covariance\n        self.R = np.eye(dim_z)\n\n        # State transition matrix\n        self.F = np.eye(dim_x)\n\n        # Measurement function matrix\n        self.H = np.zeros((dim_z, dim_x))\n\n        # Control transition matrix\n        self.B = np.zeros((dim_x, dim_u)) if dim_u > 0 else 0\n\n    def predict(self, u=None):\n        """\n        Predict next state\n        u: control input\n        """\n        # State prediction: x = F*x + B*u\n        if u is not None:\n            self.x = np.dot(self.F, self.x) + np.dot(self.B, u)\n        else:\n            self.x = np.dot(self.F, self.x)\n\n        # Covariance prediction: P = F*P*F^T + Q\n        self.P = np.dot(np.dot(self.F, self.P), self.F.T) + self.Q\n\n    def update(self, z):\n        """\n        Update state estimate with measurement\n        z: measurement vector\n        """\n        # Innovation: y = z - H*x\n        y = z - np.dot(self.H, self.x)\n\n        # Innovation covariance: S = H*P*H^T + R\n        S = np.dot(np.dot(self.H, self.P), self.H.T) + self.R\n\n        # Kalman gain: K = P*H^T*S^(-1)\n        K = np.dot(np.dot(self.P, self.H.T), np.linalg.inv(S))\n\n        # State update: x = x + K*y\n        self.x = self.x + np.dot(K, y)\n\n        # Covariance update: P = (I - K*H)*P\n        I = np.eye(self.dim_x)\n        self.P = np.dot((I - np.dot(K, self.H)), self.P)\n\n# Example: 1D position tracking with Kalman filter\nkf = KalmanFilter(dim_x=2, dim_z=1)  # Position and velocity, 1D measurement\n\n# Initialize state: [position, velocity]\nkf.x = np.array([[0.0], [0.0]])\n\n# State transition matrix (constant velocity model)\ndt = 0.1  # time step\nkf.F = np.array([[1, dt],\n                 [0, 1]])\n\n# Measurement function (only position is measured)\nkf.H = np.array([[1, 0]])\n\n# Process noise (how much we expect the model to be wrong)\nkf.Q = np.array([[0.01, 0],\n                 [0, 0.1]])\n\n# Measurement noise (sensor noise)\nkf.R = np.array([[0.1]])\n\n# Simulate measurements and track\ntrue_positions = []\nmeasurements = []\nestimates = []\ntimes = []\n\nfor t in np.arange(0, 10, dt):\n    # Simulate true motion (simple acceleration)\n    true_pos = 0.1 * t**2  # Quadratic motion\n    true_vel = 0.2 * t\n\n    # Add noise to measurement\n    measured_pos = true_pos + np.random.normal(0, 0.1)\n\n    # Predict\n    kf.predict()\n\n    # Update with measurement\n    kf.update(np.array([[measured_pos]]))\n\n    # Store results\n    true_positions.append(true_pos)\n    measurements.append(measured_pos)\n    estimates.append(kf.x[0, 0])\n    times.append(t)\n\nprint(f"Kalman filter example completed with {len(times)} time steps")\nprint(f"Final estimate: Position={kf.x[0,0]:.3f}, Velocity={kf.x[1,0]:.3f}")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"32-extended-kalman-filter-ekf",children:"3.2 Extended Kalman Filter (EKF)"}),"\n",(0,s.jsx)(n.p,{children:"For nonlinear systems, the Extended Kalman Filter linearizes around the current estimate."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class ExtendedKalmanFilter:\n    def __init__(self, dim_x, dim_z, dim_u=0):\n        """\n        Extended Kalman Filter for nonlinear systems\n        """\n        self.dim_x = dim_x\n        self.dim_z = dim_z\n        self.dim_u = dim_u\n\n        self.x = np.zeros((dim_x, 1))  # State\n        self.P = np.eye(dim_x) * 1000  # Covariance\n        self.Q = np.eye(dim_x)         # Process noise\n        self.R = np.eye(dim_z)         # Measurement noise\n\n    def predict_and_jacobian(self, fx, FJacobian, u=None):\n        """\n        Predict step with Jacobian of nonlinear function\n        """\n        # Update state: x = fx(x, u)\n        self.x = fx(self.x, u)\n\n        # Get Jacobian of state transition function\n        F = FJacobian(self.x, u)\n\n        # Update covariance: P = F*P*F^T + Q\n        self.P = np.dot(np.dot(F, self.P), F.T) + self.Q\n\n    def update_and_jacobian(self, hx, HJacobian, z):\n        """\n        Update step with Jacobian of measurement function\n        """\n        # Get measurement prediction\n        z_pred = hx(self.x)\n\n        # Get Jacobian of measurement function\n        H = HJacobian(self.x)\n\n        # Innovation: y = z - z_pred\n        y = z - z_pred\n\n        # Innovation covariance: S = H*P*H^T + R\n        S = np.dot(np.dot(H, self.P), H.T) + self.R\n\n        # Kalman gain: K = P*H^T*S^(-1)\n        K = np.dot(np.dot(self.P, H.T), np.linalg.inv(S))\n\n        # State update: x = x + K*y\n        self.x = self.x + np.dot(K, y)\n\n        # Covariance update: P = (I - K*H)*P\n        I = np.eye(self.dim_x)\n        self.P = np.dot((I - np.dot(K, H)), self.P)\n\n# Example: Nonlinear tracking with EKF (tracking in polar coordinates)\ndef fx_radar(state, dt):\n    """Nonlinear state transition function for radar tracking"""\n    x, y, vx, vy = state.flatten()\n\n    # Update position based on velocity\n    new_x = x + vx * dt\n    new_y = y + vy * dt\n    new_vx = vx  # Assume constant velocity model\n    new_vy = vy\n\n    return np.array([[new_x], [new_y], [new_vx], [new_vy]])\n\ndef FJacobian_radar(state, dt):\n    """Jacobian of state transition function"""\n    return np.array([\n        [1, 0, dt, 0],\n        [0, 1, 0, dt],\n        [0, 0, 1, 0],\n        [0, 0, 0, 1]\n    ])\n\ndef hx_radar(state):\n    """Measurement function: convert cartesian to polar"""\n    x, y, _, _ = state.flatten()\n\n    # Convert to range and bearing\n    rng = np.sqrt(x**2 + y**2)\n    bearing = np.arctan2(y, x)\n\n    return np.array([[rng], [bearing]])\n\ndef HJacobian_radar(state):\n    """Jacobian of measurement function"""\n    x, y, _, _ = state.flatten()\n    r = np.sqrt(x**2 + y**2)\n\n    return np.array([\n        [x/r, y/r, 0, 0],           # dr/dx, dr/dy, dr/dvx, dr/dvy\n        [-y/(r**2), x/(r**2), 0, 0] # db/dx, db/dy, db/dvx, db/dvy\n    ])\n\n# Initialize EKF for radar tracking\nekf = ExtendedKalmanFilter(dim_x=4, dim_z=2)  # x, y, vx, vy -> range, bearing\n\n# Initial state (position and velocity)\nekf.x = np.array([[1000.0], [0.0], [0.0], [100.0]])  # Moving north at 100 m/s\n\n# Covariances\nekf.Q = np.eye(4) * 0.1  # Process noise\nekf.R = np.array([[5**2, 0], [0, 0.01**2]])  # Measurement noise (range and bearing)\n\nprint(f"Extended Kalman Filter initialized for radar tracking")\nprint(f"Initial state: Position=({ekf.x[0,0]:.1f}, {ekf.x[1,0]:.1f}), "\n      f"Velocity=({ekf.x[2,0]:.1f}, {ekf.x[3,0]:.1f})")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"33-particle-filter",children:"3.3 Particle Filter"}),"\n",(0,s.jsx)(n.p,{children:"For highly nonlinear systems with non-Gaussian noise, particle filters can be more appropriate."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class ParticleFilter:\n    def __init__(self, num_particles, dim_state):\n        """\n        Particle Filter implementation\n        """\n        self.num_particles = num_particles\n        self.dim_state = dim_state\n\n        # Initialize particles randomly\n        self.particles = np.random.randn(num_particles, dim_state)\n        self.weights = np.ones(num_particles) / num_particles\n\n    def predict(self, process_noise_std):\n        """\n        Predict step: propagate particles through motion model\n        """\n        # Add random noise to each particle\n        noise = np.random.normal(0, process_noise_std, self.particles.shape)\n        self.particles += noise\n\n    def update(self, measurement, measurement_std, measurement_model):\n        """\n        Update step: weight particles based on measurement likelihood\n        """\n        # Calculate likelihood of measurement for each particle\n        for i, particle in enumerate(self.particles):\n            predicted_measurement = measurement_model(particle)\n\n            # Calculate likelihood (assuming Gaussian noise)\n            likelihood = np.exp(-0.5 * ((measurement - predicted_measurement) / measurement_std)**2)\n\n            # Update weight\n            self.weights[i] *= likelihood\n\n        # Normalize weights\n        self.weights += 1.e-300  # Avoid division by zero\n        self.weights /= np.sum(self.weights)\n\n    def resample(self):\n        """\n        Resample particles based on weights\n        """\n        # Systematic resampling\n        indices = []\n        cumulative_sum = np.cumsum(self.weights)\n        start = np.random.random() / self.num_particles\n\n        i, j = 0, 0\n        while i < self.num_particles:\n            if start < cumulative_sum[j]:\n                indices.append(j)\n                start += 1.0 / self.num_particles\n                i += 1\n            else:\n                j += 1\n\n        # Resample particles\n        self.particles = self.particles[indices]\n        self.weights.fill(1.0 / self.num_particles)\n\n    def estimate(self):\n        """\n        Calculate state estimate as weighted average of particles\n        """\n        return np.average(self.particles, weights=self.weights, axis=0)\n\n# Example: Simple 1D position tracking with particle filter\ndef measurement_model_1d(particle_state):\n    """Simple measurement model: just return position"""\n    return particle_state[0]  # Assume first element is position\n\npf = ParticleFilter(num_particles=1000, dim_state=2)  # Position and velocity\n\n# Set initial particles around an estimated position\npf.particles[:, 0] = np.random.normal(10.0, 2.0, pf.num_particles)  # Position\npf.particles[:, 1] = np.random.normal(1.0, 0.5, pf.num_particles)   # Velocity\n\n# Simulate tracking\ntrue_position = 10.0\nmeasurements = [9.8, 10.2, 10.1, 9.9, 10.3, 10.0, 9.7, 10.4]\n\nfor measurement in measurements:\n    # Prediction step\n    pf.predict(process_noise_std=0.1)\n\n    # Update step\n    pf.update(measurement, measurement_std=0.5, measurement_model=measurement_model_1d)\n\n    # Resample if effective sample size is low\n    neff = 1.0 / np.sum(pf.weights**2)\n    if neff < pf.num_particles / 2:\n        pf.resample()\n\n    # Get estimate\n    estimate = pf.estimate()\n    print(f"Measurement: {measurement:.2f}, Estimate: {estimate[0]:.2f}")\n\nfinal_estimate = pf.estimate()\nprint(f"Final particle filter estimate: Position={final_estimate[0]:.3f}, Velocity={final_estimate[1]:.3f}")\n'})}),"\n",(0,s.jsx)(n.h2,{id:"4-pybullet-examples-for-perception",children:"4. PyBullet Examples for Perception"}),"\n",(0,s.jsx)(n.p,{children:"Let's implement some perception examples using PyBullet."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import pybullet as p\nimport pybullet_data\nimport time\nimport numpy as np\n\ndef setup_perception_demo():\n    """Set up PyBullet environment for perception demonstration"""\n    # Connect to PyBullet\n    physicsClient = p.connect(p.GUI)\n\n    # Set gravity\n    p.setGravity(0, 0, -9.81)\n\n    # Load plane\n    p.setAdditionalSearchPath(pybullet_data.getDataPath())\n    planeId = p.loadURDF("plane.urdf")\n\n    # Add some objects for the robot to perceive\n    boxId = p.loadURDF("cube.urdf", [2, 0, 0.5], useFixedBase=False)\n    sphereId = p.loadURDF("sphere2.urdf", [-1, 1, 0.5], useFixedBase=False)\n\n    return physicsClient, boxId, sphereId\n\ndef demo_ray_casting_perception():\n    """\n    Demonstrate ray casting for distance sensing\n    This simulates how a robot might use LIDAR or similar sensors\n    """\n    # Set up environment\n    physicsClient, boxId, sphereId = setup_perception_demo()\n\n    # Define robot position and multiple ray directions (simulating LIDAR)\n    robot_pos = [0, 0, 1]\n    ray_directions = []\n\n    # Create rays in a circular pattern around the robot\n    for angle in np.linspace(0, 2*np.pi, 36):  # 36 rays\n        direction = [np.cos(angle), np.sin(angle), 0]\n        ray_directions.append(direction)\n\n    # Perform ray casting\n    ray_starts = [robot_pos for _ in ray_directions]\n    ray_ends = [[robot_pos[0] + 10*d[0], robot_pos[1] + 10*d[1], robot_pos[2] + 10*d[2]]\n                for d in ray_directions]\n\n    results = p.rayTestBatch(ray_starts, ray_ends)\n\n    # Process results\n    distances = []\n    for i, result in enumerate(results):\n        hit_fraction = result[2]  # Fraction of ray length where hit occurred\n        if hit_fraction == 1.0:\n            # No hit, maximum range\n            distances.append(10.0)  # Maximum range\n        else:\n            # Calculate actual distance\n            distance = hit_fraction * 10.0\n            distances.append(distance)\n\n    print(f"Ray casting completed: {len(distances)} distance measurements")\n    print(f"Min distance: {min(distances):.2f}m, Max distance: {max(distances):.2f}m")\n\n    # Run simulation briefly to visualize\n    for i in range(300):\n        p.stepSimulation()\n        time.sleep(1./240.)\n\n    p.disconnect()\n\n    return distances\n\n# Note: This example would run in an environment with PyBullet installed\nprint("PyBullet perception demo defined - requires PyBullet installation to run")\n'})}),"\n",(0,s.jsx)(n.h2,{id:"5-sensor-fusion-exercise",children:"5. Sensor Fusion Exercise"}),"\n",(0,s.jsx)(n.h3,{id:"exercise-1-multi-sensor-position-estimation",children:"Exercise 1: Multi-Sensor Position Estimation"}),"\n",(0,s.jsx)(n.p,{children:"Implement a system that fuses GPS, IMU, and wheel encoder data for position estimation."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class MultiSensorFusion:\n    def __init__(self):\n        """\n        Multi-sensor fusion system combining GPS, IMU, and wheel encoders\n        """\n        # Initialize Kalman filter for position and velocity\n        self.kf = KalmanFilter(dim_x=4, dim_z=3)  # [x, y, vx, vy] state, [x_gps, y_gps, theta_imu] measurement\n\n        # Initial state: [x, y, vx, vy]\n        self.kf.x = np.array([[0.0], [0.0], [0.0], [0.0]])\n\n        # State transition model (constant velocity)\n        dt = 0.1\n        self.kf.F = np.array([\n            [1, 0, dt, 0],\n            [0, 1, 0, dt],\n            [0, 0, 1, 0],\n            [0, 0, 0, 1]\n        ])\n\n        # Measurement function: we can measure x, y position and heading\n        self.kf.H = np.array([\n            [1, 0, 0, 0],  # Measure x position\n            [0, 1, 0, 0],  # Measure y position\n            [0, 0, 0, 0]   # We\'ll handle heading separately\n        ])\n\n        # Process noise\n        self.kf.Q = np.array([\n            [0.01, 0, 0, 0],\n            [0, 0.01, 0, 0],\n            [0, 0, 0.1, 0],\n            [0, 0, 0, 0.1]\n        ])\n\n        # Measurement noise\n        self.kf.R = np.array([\n            [1.0, 0, 0],    # GPS position noise\n            [0, 1.0, 0],    # GPS position noise\n            [0, 0, 0.01]    # IMU heading noise\n        ])\n\n        self.dt = dt\n        self.time = 0\n\n    def update_with_sensors(self, gps_pos, imu_yaw, wheel_odom_delta):\n        """\n        Update the state estimate using multiple sensor inputs\n        """\n        # Prediction step\n        self.kf.predict()\n        self.time += self.dt\n\n        # Create measurement vector\n        # For this example, we\'ll use GPS position and IMU heading\n        measurement = np.array([[gps_pos[0]], [gps_pos[1]], [imu_yaw]])\n\n        # Update with measurement\n        self.kf.update(measurement)\n\n        return self.kf.x.flatten()\n\n# Example: Simulate multi-sensor fusion\nfusion_system = MultiSensorFusion()\n\n# Simulate sensor data\nfor step in range(50):\n    # Simulate true motion\n    true_x = 0.1 * step * np.cos(step * 0.1)  # Spiral motion\n    true_y = 0.1 * step * np.sin(step * 0.1)\n    true_heading = step * 0.05  # Gradually changing heading\n\n    # Simulate noisy sensor readings\n    gps_pos = [true_x + np.random.normal(0, 0.5), true_y + np.random.normal(0, 0.5)]\n    imu_yaw = true_heading + np.random.normal(0, 0.01)\n    wheel_odom_delta = [0.1, 0.01]  # Simulated wheel encoder data\n\n    # Update fusion system\n    state_estimate = fusion_system.update_with_sensors(gps_pos, imu_yaw, wheel_odom_delta)\n\n    if step % 10 == 0:  # Print every 10 steps\n        print(f"Step {step}: Estimated pos=({state_estimate[0]:.2f}, {state_estimate[1]:.2f}), "\n              f"True pos=({true_x:.2f}, {true_y:.2f})")\n\nfinal_state = fusion_system.kf.x.flatten()\nprint(f"Final fused state: x={final_state[0]:.3f}, y={final_state[1]:.3f}, "\n      f"vx={final_state[2]:.3f}, vy={final_state[3]:.3f}")\n'})}),"\n",(0,s.jsx)(n.h2,{id:"6-summary",children:"6. Summary"}),"\n",(0,s.jsx)(n.p,{children:"This chapter covered the essential aspects of sensing and perception in robotics:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sensor Types"}),": Understanding proprioceptive (encoders, IMUs) and exteroceptive (range sensors, cameras) sensors"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sensor Characteristics"}),": Learning about accuracy, precision, range, field of view, and noise characteristics"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sensor Fusion"}),": Combining information from multiple sensors to improve perception"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"State Estimation"}),": Using filtering techniques (Kalman, Extended Kalman, Particle) to maintain accurate state estimates"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Practical Implementation"}),": Using PyBullet for perception simulation"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Effective sensing and perception are fundamental to Physical AI systems, enabling robots to understand their environment and make intelligent decisions. The fusion of multiple sensor modalities is crucial for robust operation in real-world conditions."}),"\n",(0,s.jsx)(n.h2,{id:"7-implementation-guide",children:"7. Implementation Guide"}),"\n",(0,s.jsx)(n.p,{children:"To implement the sensing and perception concepts covered in this chapter:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Experiment with different sensor models and their characteristics"}),"\n",(0,s.jsx)(n.li,{children:"Implement sensor fusion algorithms to combine multiple sensor inputs"}),"\n",(0,s.jsx)(n.li,{children:"Practice state estimation using various filtering techniques"}),"\n",(0,s.jsx)(n.li,{children:"Use PyBullet to simulate sensor data and test perception algorithms"}),"\n",(0,s.jsx)(n.li,{children:"Apply probabilistic methods to handle uncertainty in sensor data"}),"\n",(0,s.jsx)(n.li,{children:"Design perception systems tailored to specific robotic applications"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The exercises provided offer hands-on practice with these fundamental concepts, preparing readers for more advanced topics in robotics and Physical AI."})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453(e,n,i){i.d(n,{R:()=>r,x:()=>o});var t=i(6540);const s={},a=t.createContext(s);function r(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);